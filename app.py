import pandas as pd
import streamlit as st
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from langchain.callbacks.base import BaseCallbackHandler
from html_template import logo
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
import re

# Load environment variables
load_dotenv()

# Get API key from environment variables
API_KEY = os.getenv("OPENAI_API_KEY")
if API_KEY is None:
    st.error("Error: OPENAI_API_KEY not found in environment variables")
    st.stop()
    
st.set_page_config(page_title="AlcaldIA", layout="centered", menu_items= {
        'Get Help': 'https://www.extremelycoolapp.com/help',
        'Report a bug': "https://www.extremelycoolapp.com/bug",
        'About': "# This is a header. This is an *extremely* cool app!"
    })

class MunicipalDocumentProcessor:
    def __init__(self, pdf_directory="data", index_directory="faiss_index"):
        self.pdf_directory = pdf_directory
        self.index_directory = index_directory
        self.embeddings = OpenAIEmbeddings()
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )
        
        os.makedirs(self.pdf_directory, exist_ok=True)
        os.makedirs(self.index_directory, exist_ok=True)

    def load_vector_store(self):
        """Carga el vector store existente"""
        index_path = os.path.join(self.index_directory, "index.faiss")
        try:
            if os.path.exists(index_path):
                return FAISS.load_local(
                    self.index_directory, 
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
            return None
        except Exception as e:
            st.error(f"Error cargando vector store: {str(e)}")
            return None

    def process_documents(self):
        """Procesa los documentos PDF y crea el vector store"""
        try:
            pdf_files = [f for f in os.listdir(self.pdf_directory) if f.endswith('.pdf')]
            if not pdf_files:
                st.warning("No se encontraron archivos PDF en el directorio data.")
                return None

            documents = []
            successful_files = []
            failed_files = []
            
            for pdf_file in pdf_files:
                try:
                    file_path = os.path.join(self.pdf_directory, pdf_file)
                    
                    with open(file_path, 'rb') as file:
                        header = file.read(5)
                        if header != b'%PDF-':
                            failed_files.append((pdf_file, "Encabezado PDF inv√°lido"))
                            continue
                    
                    loader = PyPDFLoader(file_path)
                    doc_pages = loader.load()
                    
                    if doc_pages:
                        documents.extend(doc_pages)
                        successful_files.append(pdf_file)
                        st.success(f"‚úÖ Procesado exitosamente: {pdf_file} ({len(doc_pages)} p√°ginas)")
                    else:
                        failed_files.append((pdf_file, "No se pudo extraer contenido"))
                
                except Exception as e:
                    failed_files.append((pdf_file, str(e)))
                    continue

            st.write("---")
            st.write("üìä Resumen de procesamiento:")
            st.write(f"- Total archivos: {len(pdf_files)}")
            st.write(f"- Procesados correctamente: {len(successful_files)}")
            st.write(f"- Fallidos: {len(failed_files)}")
            
            if failed_files:
                st.error("‚ùå Archivos que no se pudieron procesar:")
                for file, error in failed_files:
                    st.write(f"- {file}: {error}")

            if not documents:
                st.warning("‚ö†Ô∏è No se pudo extraer contenido de ning√∫n PDF.")
                return None

            texts = self.text_splitter.split_documents(documents)
            vectorstore = FAISS.from_documents(texts, self.embeddings)
            vectorstore.save_local(self.index_directory)
            
            st.success(f"‚úÖ Vector store creado exitosamente con {len(texts)} fragmentos de texto")
            return vectorstore
        
        except Exception as e:
            st.error(f"Error procesando documentos: {str(e)}")
            return None

def setup_retrieval_chain(vector_store):
    """Configura la cadena de recuperaci√≥n para consultas"""
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True
    )
    
    retrieval_chain = ConversationalRetrievalChain.from_llm(
        llm=ChatOpenAI(temperature=0),
        retriever=vector_store.as_retriever(search_kwargs={"k": 3}),
        memory=memory,
        return_source_documents=True
    )
    
    return retrieval_chain

class StreamHandler(BaseCallbackHandler):
    def __init__(self, container):
        self.container = container
        self.text = ""
        
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)

def get_municipal_context(vector_store, query):
    """
    Obtiene el contexto relevante de los documentos para una consulta.
    """
    similar_docs = vector_store.similarity_search(query, k=3)
    context_list = []
    
    for doc in similar_docs:
        # Extraer informaci√≥n b√°sica del documento
        content = doc.page_content
        source = doc.metadata.get('source', 'Documento sin especificar')
        
        # Extraer datos cuantitativos
        numbers = re.findall(r'(\d+(?:\.\d+)?(?:%|\s+(?:habitantes|personas|viviendas)))', content)
        metrics = numbers[:3] if numbers else []
        
        # Extraer referencias a pol√≠ticas, programas o indicadores
        refs = re.findall(r'(?:Plan|Programa|Proyecto|Meta|Indicador)[\s:].*?(?=\n|$)', content)
        
        context_list.append({
            'source': source,
            'content': content[:300],  # Limitar longitud del contenido
            'metrics': metrics,
            'refs': refs[:3]  # Limitar n√∫mero de referencias
        })
    
    return context_list

SYSTEM_PROMPT = """
Eres BogotAI, un asistente especializado para apoyar al equipo de la Alcald√≠a de Bogot√°. Tu funci√≥n es proporcionar informaci√≥n precisa basada EXCLUSIVAMENTE en los documentos oficiales, principalmente el Plan de Desarrollo.

ESTRUCTURA DE RESPUESTA:
Para cada consulta, debes estructurar tu respuesta de la siguiente manera:

1. DIAGN√ìSTICO üìä
- Situaci√≥n actual seg√∫n los documentos oficiales
- Problem√°ticas identificadas
- L√≠nea base de indicadores
[Citar p√°gina y documento espec√≠fico]

2. OBJETIVOS Y ESTRATEGIA üéØ
- Objetivos espec√≠ficos del Plan de Desarrollo
- Estrategias planteadas
- Programas relacionados
[Citar p√°gina y documento espec√≠fico]

3. CIFRAS RELEVANTES üìà
- Indicadores clave
- Datos de l√≠nea base
- Proyecciones establecidas
[Citar p√°gina y documento espec√≠fico]

4. METAS ESTRAT√âGICAS ‚≠ê
- Metas espec√≠ficas del Plan de Desarrollo
- Indicadores de seguimiento
- Hitos clave
[Citar p√°gina y documento espec√≠fico]

5. PRESUPUESTO üí∞
- Asignaci√≥n presupuestal
- Fuentes de financiaci√≥n
- Distribuci√≥n por componentes
[Citar p√°gina y documento espec√≠fico]

6. DOCUMENTOS DE PLANEACI√ìN üìë
- Referencias a otros documentos oficiales
- Articulaci√≥n con otras pol√≠ticas
- Marco normativo relacionado
[Citar documento espec√≠fico]

Ejemplo de respuesta:
"Seg√∫n el Plan de Desarrollo (p√°g. 45), los objetivos para comedores comunitarios son:
1. Construcci√≥n de 2000 comedores nuevos
2. Focalizaci√≥n de 30.000 a 20.000 ciudadanos adicionales
[...]"

## Advertencias y Limitaciones

- Indica claramente cuando la informaci√≥n est√© desactualizada
- SIEMPRE citar la p√°gina espec√≠fica y el documento de donde se obtiene la informaci√≥n
- NO realizar interpretaciones o inferencias fuera de los documentos
- Se√±ala √°reas donde falten datos o evidencia
- Especifica cuando las recomendaciones sean preliminares
- Sugiere la consulta con expertos cuando sea necesaria  
- Si no tienes informacion sobre algo en especifico, responde con que no tienes suficiente informacion sobre eso o neesitas mas informacion sobre eso. 
- SIEMPRE RESPONDE EN ESPA√ëOL  
- Mantener la objetividad y ce√±irse estrictamente a lo establecido en los documentos
- Si te saludan "Hola BogotAI" o preguntan quien eres respondeles de manera concisas diciendo quien ers y en que puedes ayudarlos. 

Recuerda: Tu rol es apoyar la toma de decisiones proporcionando informaci√≥n y an√°lisis basado en evidencia, no tomar las decisiones finales.
"""

def detect_response_format(prompt):
    """
    Detecta si una consulta requiere una respuesta estructurada o simple.
    Retorna un string con el formato detectado.
    
    Parameters:
    prompt (str): La consulta del usuario
    
    Returns:
    str: 'STRUCTURED' o 'SIMPLE'
    """
    prompt = prompt.lower()
    
    # Indicadores de consulta estructurada
    structured_indicators = [
        # An√°lisis y comparaci√≥n
        'analizar', 'comparar', 'evaluar', 'diferencia',
        'evoluci√≥n', 'tendencia', 'impacto',
        
        # Planeaci√≥n y gesti√≥n
        'plan', 'programa', 'proyecto', 'estrategia',
        'pol√≠tica', 'presupuesto', 'implementaci√≥n',
        
        # Territorio y datos
        'localidad', 'territorio', 'zona', 'sector',
        'estad√≠stica', 'indicador', 'porcentaje', 'densidad',
        
        # Tem√°ticas complejas
        'seguridad', 'movilidad', 'pobreza', 'desarrollo',
        'infraestructura', 'ambiente', 'educaci√≥n', 'salud'
    ]
    
    # Indicadores de consulta simple
    simple_indicators = [
        # Preguntas b√°sicas
        'qu√© es', 'que es', 'd√≥nde', 'donde', 'cu√°ndo', 'cuando',
        'qui√©n', 'quien', 'cu√°l', 'cual', 'cu√°nto', 'cuanto',
        
        # Definiciones y datos puntuales
        'significa', 'define', 'explica', 'valor', 'dato',
        'horario', 'direcci√≥n', 'tel√©fono', 'requisito'
    ]
    
    # Criterios de complejidad
    is_complex = (
        len(prompt.split()) > 15 or              # Longitud de la pregunta
        prompt.count('?') > 1 or                 # M√∫ltiples preguntas
        prompt.count(',') > 1 or                 # M√∫ltiples elementos
        prompt.count(' y ') > 1 or              # M√∫ltiples conceptos
        any(ind in prompt for ind in structured_indicators)  # Indicadores de estructura
    )
    
    # Criterios de simplicidad
    is_simple = (
        any(ind in prompt for ind in simple_indicators) and  # Indicadores simples
        not is_complex                                       # No es compleja
    )
    
    return 'SIMPLE' if is_simple else 'STRUCTURED'

def format_context_string(context_list):
    """
    Formatea la lista de contextos enfatizando las referencias a documentos oficiales
    """
    if not context_list:
        return "No se encontr√≥ informaci√≥n relevante en los documentos oficiales."
        
    formatted_parts = []
    
    for item in context_list:
        section = f"""
üìö Fuente: {item['source']}
[Referencia espec√≠fica del documento oficial]

üìä Datos oficiales:
{format_metrics(item.get('metrics', []))}

üìã Referencias en Plan de Desarrollo y documentos relacionados:
{format_references(item.get('refs', []))}

üí° Contexto oficial:
{item['content']}"""
        formatted_parts.append(section)
    
    return "\n---\n".join(formatted_parts)

def format_simple_response(query_type, context):
    """Genera un prompt para respuesta simple"""
    return f"""
    Tipo de consulta: {query_type}
    
    Contexto municipal relevante:
    {context}
    
    Proporciona una respuesta clara y concisa en formato de p√°rrafo, sin usar vi√±etas ni secciones.
    La respuesta debe ser directa y enfocada en responder la pregunta espec√≠fica.
    """

def format_municipal_context(context_list):
    """
    Formatea el contexto municipal para presentaci√≥n.
    """
    if not isinstance(context_list, list):
        return "No se encontr√≥ contexto relevante."
        
    formatted_parts = []
    
    for item in context_list:
        # Formatear referencias
        refs = item.get('refs', [])
        refs_text = "\n‚Ä¢ ".join(refs) if refs else "No hay referencias espec√≠ficas"
        
        # Formatear m√©tricas
        metrics = item.get('metrics', [])
        metrics_text = "\n‚Ä¢ ".join(metrics) if metrics else "No hay datos cuantitativos espec√≠ficos"
        
        section = f"""
üìö Fuente: {item['source']}

üìä Datos clave:
‚Ä¢ {metrics_text}

üìã Referencias:
‚Ä¢ {refs_text}

üí° Contexto relevante:
{item['content']}"""
        formatted_parts.append(section)
    
    return "\n---\n".join(formatted_parts)

def detect_query_type(prompt):
    """
    Detecta el tipo de consulta basado en los ejes principales del Plan de 
    Desarrollo de Bogot√° y prioridades de la administraci√≥n.
    
    Parameters:
    prompt (str): Consulta del usuario
    
    Returns:
    tuple: (tipo_principal, subtipo, score)
    """
    prompt = prompt.lower()
    
    keywords = {
        'SEGURIDAD_MOVILIDAD': [
            # Seguridad
            'seguridad', 'convivencia', 'delito', 'crimen', 'polic√≠a',
            'vigilancia', 'prevenci√≥n', 'violencia', 'hurto',
            # Movilidad
            'transporte', 'metro', 'transmilenio', 'ciclov√≠a', 'tr√°fico',
            'congesti√≥n', 'obras viales', 'infraestructura vial', 'peatones'
        ],
        
        'EQUIDAD_SOCIAL': [
            # Pobreza y desigualdad
            'pobreza', 'vulnerabilidad', 'inequidad', 'brecha social',
            'transferencias', 'subsidios', 'ayudas', 'inclusi√≥n',
            # Servicios sociales
            'educaci√≥n', 'salud', 'vivienda', 'alimentaci√≥n', 'cuidado',
            'primera infancia', 'adulto mayor', 'discapacidad', 'g√©nero'
        ],
        
        'PLANEACION_TERRITORIO': [
            # Planeaci√≥n
            'plan de desarrollo', 'pot', 'ordenamiento', 'planeaci√≥n',
            'estrategia', 'proyecto', 'programa', 'pol√≠tica p√∫blica',
            # Territorio
            'localidad', 'upz', 'territorio', 'densidad', 'uso del suelo',
            'espacio p√∫blico', 'equipamientos', 'regi√≥n metropolitana'
        ],
        
        'GESTION_RECURSOS': [
            # Gesti√≥n p√∫blica
            'presupuesto', 'inversi√≥n', 'recursos', 'contrataci√≥n',
            'ejecuci√≥n', 'gesti√≥n', 'administrativo', 'modernizaci√≥n',
            # Control
            'seguimiento', 'indicadores', 'evaluaci√≥n', 'metas',
            'transparencia', 'rendici√≥n', 'control', 'auditor√≠a'
        ],
        
        'AMBIENTE_DESARROLLO': [
            # Ambiente
            'ambiente', 'sostenibilidad', 'cambio clim√°tico', 'residuos',
            'reciclaje', 'contaminaci√≥n', 'aire', 'agua', 'verde',
            # Desarrollo econ√≥mico
            'econom√≠a', 'emprendimiento', 'empleo', 'productividad',
            'innovaci√≥n', 'tecnolog√≠a', 'competitividad', 'mipymes'
        ],
        
        'SERVICIOS_CIUDADANOS': [
            # Tr√°mites
            'tr√°mite', 'servicio', 'procedimiento', 'requisitos',
            'documentos', 'solicitud', 'atenci√≥n', 'ventanilla',
            # Participaci√≥n
            'participaci√≥n', 'consulta', 'ciudadan√≠a', 'comunidad',
            'socializaci√≥n', 'encuentro', 'di√°logo', 'concertaci√≥n'
        ]
    }
    
    prompt_lower = prompt.lower()
    for category, terms in keywords.items():
        if any(term in prompt_lower for term in terms):
            return category
    return 'GENERAL'

def format_context_string(context_list):
    """
    Formatea la lista de contextos en un string estructurado.
    """
    if not context_list:
        return "No se encontr√≥ contexto relevante."
        
    formatted_parts = []
    
    for item in context_list:
        section = f"""
üìö Fuente: {item['source']}

üìä Datos relevantes:
{format_metrics(item.get('metrics', []))}

üìã Referencias:
{format_references(item.get('refs', []))}

üí° Contexto:
{item['content']}"""
        formatted_parts.append(section)
    
    return "\n---\n".join(formatted_parts)

def format_metrics(metrics):
    if not metrics:
        return "‚Ä¢ No hay datos cuantitativos espec√≠ficos"
    return "\n".join(f"‚Ä¢ {metric}" for metric in metrics)

def format_references(refs):
    if not refs:
        return "‚Ä¢ No hay referencias espec√≠ficas"
    return "\n".join(f"‚Ä¢ {ref}" for ref in refs)

def get_chat_response(prompt, vector_store, temperature=0.3):
    """Genera respuesta considerando el contexto municipal y el formato apropiado"""
    try:
        response_placeholder = st.empty()
        stream_handler = StreamHandler(response_placeholder)
        
        # 1. Detectar tipo de consulta
        query_type = detect_query_type(prompt)
        
        # 2. Obtener y formatear contexto
        context_items = get_municipal_context(vector_store, prompt)
        formatted_context = format_context_string(context_items)
        
        # 3. Construir prompt mejorado
        enhanced_prompt = f"""
Tipo de consulta: {query_type}

Contexto relevante:
{formatted_context}

Por favor proporciona una respuesta que:
1. Siga ESTRICTAMENTE la estructura definida (Diagn√≥stico, Objetivos, Cifras, etc.)
2. Cite espec√≠ficamente las p√°ginas y documentos fuente
3. Se base √öNICAMENTE en la informaci√≥n disponible en los documentos oficiales
4. Indique expl√≠citamente cuando no haya informaci√≥n disponible sobre alg√∫n aspecto
"""
        
        # 4. Generar respuesta
        chat_model = ChatOpenAI(
            model="gpt-4o",
            temperature=temperature,
            api_key=API_KEY,
            streaming=True,
            callbacks=[stream_handler]
        )
        
        messages = [
            SystemMessage(content=SYSTEM_PROMPT),
            HumanMessage(content=f"{prompt}\n\n{enhanced_prompt}")
        ]
        
        response = chat_model.invoke(messages)
        return stream_handler.text
            
    except Exception as e:
        st.error(f"Error generando respuesta: {str(e)}")
        return "Lo siento, ocurri√≥ un error al procesar su solicitud."

def main():
    processor = MunicipalDocumentProcessor()
    
    if os.path.exists(os.path.join("faiss_index", "index.faiss")):
        vector_store = processor.load_vector_store()
    else:
        st.warning("Procesando documentos municipales por primera vez...")
        vector_store = processor.process_documents()
    
    if vector_store is None:
        st.error("No se pudo inicializar la base de conocimientos")
        st.stop()

    st.write(logo, unsafe_allow_html=True)
    st.title("Alcald-IA", anchor=False)
    st.markdown("##### Asistente virtual para la toma de decisiones de alto gobierno distrital")
    
    if "messages" not in st.session_state:
        st.session_state.messages = []

    with st.sidebar:
        st.markdown("""
        ## Sistema de Gesti√≥n P√∫blica Bogot√°
        
        **Tipos de consultas:**
        - Planificaci√≥n y dise√±o de Pol√≠ticas P√∫blicas 
        - An√°lisis de problemas sociales 
        - Evaluaci√≥n de escenarios 
        - Identificaci√≥n de patrones y tendencias 
        """) 
        temperature = st.slider("Temperatura", min_value=0.1, max_value=1.0, value=0.4, step=0.1)
        
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("¬øEn qu√© puedo ayudarte?"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user", avatar="üë§"):
            st.markdown(prompt)
        
        with st.chat_message("assistant"):
            response = get_chat_response(prompt, vector_store, temperature)
            st.session_state.messages.append({"role": "assistant", "content": response})

if __name__ == "__main__":
    main()